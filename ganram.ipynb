{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ganram\n",
    "# GAN-BASED DATA AUGMENTATION for RAMAN SPECTRA\n",
    "\n",
    "#### (KRG group: https://myweb.uoi.gr/nkourkou/)\n",
    "\n",
    "#### * in this notebook (opt) denotes a parameter that can be optimised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "project ganram/\n",
    "│   readme.md\n",
    "│   project.ipynb\n",
    "│   scavenging_PCA.ipynb\n",
    "│\n",
    "└───data/\n",
    "│       input.csv\n",
    "│   \n",
    "└───output/\n",
    "│       └───data/\n",
    "│               csv/\n",
    "│                  output_synthetic.scv\n",
    "│               evolution/\n",
    "│                        image_at_epoch_000X_0000.png (images)\n",
    "│               cp/\n",
    "│                 checkpoints_X.scv\n",
    "│               generated_samples/\n",
    "│                                generated_sample_0000 (images)\n",
    "│   \n",
    "└───models/\n",
    "│         generator_model.ckpt\n",
    "│       \n",
    "└───training_checkpoints/\n",
    "│                       ckpt-X.data-00000-of-0000X (checkpoints)\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.keras as keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU, GaussianNoise\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "from frechetdist import frdist\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_FIRST_COLUMN = True                  # Remove first column from the dataframe if it has IDs, names, etc.) \n",
    "df = pd.read_csv('data/tibia_bones_raw_1800.csv')\n",
    "\n",
    "if REMOVE_FIRST_COLUMN:\n",
    "    df = df.drop(\"Raman_shift\", axis=1)              # Drop the \"Raman_shift\" column from the dataframe\n",
    "\n",
    "print(\"Successfuly loaded the dataset\")\n",
    "df                                          # Showing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the index labels (wavelength) for later use (for plotting etc.)\n",
    "\n",
    "df.columns= df.columns.astype(float)\n",
    "column_labels = df.columns.tolist\n",
    "column_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data to numpy array\n",
    "data_raw = df.to_numpy()                    \n",
    "print(f'Raw data shape: {data_raw.shape}')  # Sanity check of shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (low pass filter) and train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the signal is very complex or noisy (like EEG etc.) we can use LPF\n",
    " \n",
    "\n",
    "# data_raw = df.to_numpy()        \n",
    "# # Getting the filter coefficients of the low-pass butterworth filter \n",
    "# b, a = signal.butter(2, 0.3, 'low', analog = False)                 # Order of filter and cutoff frequency\n",
    "# data_denoised = signal.filtfilt(b, a, data_raw)                     # Applying the filter on the data, axis=-1 (row-wise) by default \n",
    "# #data_denoised = data_raw\n",
    "\n",
    "# # Data distribution modification\n",
    "# means = np.average(data_denoised, axis=0).reshape(1, -1)            # Calculating data mean (column-wise); mean of each feature\n",
    "# std_dev  = np.std(data_denoised, axis= 0).reshape(1, -1)            # Calculating data standard deviation\n",
    "# data_processed = (data_denoised - means) / std_dev                  # Data normalization (x-u)/sigma\n",
    "# print(data_processed)\n",
    "\n",
    "# # Slicing the data\n",
    "# train_data = data_processed[:int(data_processed.shape[0]*0.8), :]                # Doing the training split\n",
    "# test_data = data_processed[int(data_processed.shape[0]*0.8):, :]                 # Doing the test split\n",
    "# print(\"Train dataset shape: {}\".format(train_data.shape))\n",
    "# print(\"Test dataset shape: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (train/test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = data_raw \n",
    "\n",
    "# Slicing the data\n",
    "\n",
    "train_data = data_processed[:int(data_processed.shape[0]*0.8), :]                # Doing the training split\n",
    "test_data = data_processed[int(data_processed.shape[0]*0.8):, :]                 # Doing the test split\n",
    "print(\"Train dataset shape: {}\".format(train_data.shape))\n",
    "print(\"Test dataset shape: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.T.index.astype(float), train_data[10])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting TensorFlow train dataset\n",
    "\n",
    "BATCH_SIZE = 4                            # (opt) Tested for our dataset to be the most suitable\n",
    "data_size = train_data.shape[0]           # Number of data_points\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(data_size).batch(BATCH_SIZE)         # Shuffle and build the train dataset\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training the GAN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100                   # Dimension of the input noise vector to the generator (opt)\n",
    "feature_dim = train_data.shape[1]                      # Dimension of each feature (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input((noise_dim)))\n",
    "      \n",
    "    # Fully Connected Layers\n",
    "    #(opt) (number of nodes can change and activation may be relu or leaky relu)\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))   \n",
    "    \n",
    "    model.add(layers.Dense(256, activation=\"leaky_relu\"))    \n",
    "    model.add(layers.Dense(feature_dim))\n",
    "    model.compile()\n",
    "    \n",
    "    print(model.output_shape)\n",
    "    assert model.output_shape == (None, feature_dim)               \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the generator\n",
    "\n",
    "generator = make_generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the untrained system to create one sample output\n",
    "\n",
    "noise = tf.random.normal([1, noise_dim])\n",
    "generated_data = generator(noise, training=False)\n",
    "generated_data_ = generated_data.numpy().reshape(-1).tolist()\n",
    "plt.plot(generated_data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    \n",
    "    # Implementing a ConvNet discriminator\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Input(shape= (feature_dim)))\n",
    "    model.add(layers.Reshape([feature_dim, 1]))\n",
    "    model.add(layers.Conv1D(kernel_size= 15, filters= 256, activation='leaky_relu'))  #(opt) (number of filters and kernel size)\n",
    "    model.add(layers.MaxPool1D())\n",
    "    model.add(layers.Dropout(0.2))                                                      #(opt) (dropout probability)\n",
    "    \n",
    "    model.add(layers.Conv1D(kernel_size= 15, filters= 128))    #(opt) (number of filters and kernel size)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "        \n",
    "    model.add(layers.MaxPool1D())\n",
    "    model.add(layers.Dropout(0.2))                                                      #(opt) (dropout probability)\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64))                                                         #(opt) (number of nodes in layer)\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the discriminator model\n",
    "\n",
    "discriminator = make_discriminator_model()          \n",
    "decision = discriminator(generated_data)            # Get real or fake for the input we just got out of the generator\n",
    "print (decision)                                \n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Losses and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of cross entropy loss\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Definining the discriminator loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    return cross_entropy(tf.ones_like(real_output), real_output) + cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "# Defining the generator loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining training optimizers\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoints saving\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Visualization and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = tf.random.normal([1, noise_dim])   # Fixed input noise distribution to monitor training effect on that distribution            \n",
    "\n",
    "def draw_training_evolution(model, epoch, noise_input= seed):\n",
    "  \"\"\"\n",
    "    Function that takes in the generator model, epoch number, and \n",
    "    does a prediction and plots the generated singal then saves it.\n",
    "  \"\"\"\n",
    "  # `training` is set to False.\n",
    "  # Thus, all layers run in inference mode (batchnorm).\n",
    "  predictions = model(noise_input, training=False)\n",
    "  \n",
    "  for i in range(predictions.shape[0]):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(predictions[i].numpy().tolist())\n",
    "    plt.savefig('output/evolution/image_at_epoch_{:04d}_{:04d}.png'.format(epoch, i))\n",
    "    plt.close()\n",
    "\n",
    "def generate_data(model, num_synthetic_to_gen=1):\n",
    "  \"\"\"\n",
    "    Function that takes in the generator model and \n",
    "    does a prediction and returns it as a numpy array.\n",
    "  \"\"\"\n",
    "  noise_input = tf.random.normal([num_synthetic_to_gen, noise_dim])\n",
    "  predictions = model(noise_input, training=False)\n",
    "  predictions = predictions.numpy()\n",
    "  return predictions\n",
    "\n",
    "def calc_accuracy(prediction):\n",
    "  \"\"\"\n",
    "    Function that takes in the some data judgements \n",
    "    from the discriminator and get the average of \n",
    "    judgements that indicate how the discriminator is fooled.\n",
    "  \"\"\"\n",
    "  prediction_clipped = tf.clip_by_value(prediction, 0.0, 1.0, name=None)\n",
    "  return tf.reduce_mean(prediction_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `tf.function` # This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(data):\n",
    "    \"\"\"\n",
    "      Function for implementing one training step \n",
    "      of the GAN model\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim], seed=1)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_data = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(data, training=True)\n",
    "      fake_output = discriminator(generated_data, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "      acc = calc_accuracy(fake_output)\n",
    "     \n",
    "        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numofEPOCHS = 10000 #(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  \"\"\"\n",
    "    Main GAN Training Function\n",
    "  \"\"\"\n",
    "  epochs_gen_losses, epochs_disc_losses, epochs_accuracies = [], [], []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    gen_losses, disc_losses, accuracies = [], [], []\n",
    "\n",
    "    for data_batch in dataset:\n",
    "      gen_loss, disc_loss, acc = train_step(data_batch)\n",
    "      accuracies.append(acc)\n",
    "      gen_losses.append(gen_loss)\n",
    "      disc_losses.append(disc_loss)\n",
    "\n",
    "    epoch_gen_loss  = np.average(gen_losses)\n",
    "    epoch_disc_loss = np.average(disc_losses)\n",
    "    epoch_accuracy = np.average(accuracies)\n",
    "    epochs_gen_losses.append(epoch_gen_loss)\n",
    "    epochs_disc_losses.append(epoch_disc_loss)\n",
    "    epochs_accuracies.append(epoch_accuracy)\n",
    "    print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "    print(\"Generator Loss: {}, Discriminator Loss: {}\".format(epoch_gen_loss, epoch_disc_loss))\n",
    "    print(\"Accuracy: {}\".format(epoch_accuracy))\n",
    "      \n",
    "    # Draw the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      draw_training_evolution(generator, epoch+1)\n",
    "        \n",
    "    # Save the model every 2 epochs for the last 2000 epochs\n",
    "    if (epoch + 1) % 2 == 0 and epoch > (numofEPOCHS - 2000):\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)   # Comment not to save model checkpoints while training\n",
    "      \n",
    "\n",
    "  return epochs_gen_losses, epochs_disc_losses, epochs_accuracies   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = numofEPOCHS\n",
    "\n",
    "epochs_gen_losses, epochs_disc_losses, epochs_accuracies = train(train_dataset, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model and Calculation of Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Training Curves (Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.rcParams.update({'font.size': 20}) # must be set on top\n",
    "ax = pd.DataFrame(\n",
    "    {\n",
    "        'Generative Loss': epochs_gen_losses,\n",
    "        'Discriminative Loss': epochs_disc_losses,\n",
    "    }\n",
    ").plot(title='Training loss', logy=True, figsize=(18,12))\n",
    "ax.set_xlabel(\"Epochs\", fontsize=18)\n",
    "ax.set_ylabel(\"Loss\", fontsize=18)\n",
    "\n",
    "# Save figure using 600 dpi\n",
    "plt.savefig(\"training.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "generator.save('models/generator_model23o.ckpt', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(synthetic_datapoint, original_datapoint):\n",
    "    \"\"\"\n",
    "        Function that calculates the RMS between two datapoints\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.average((synthetic_datapoint - original_datapoint)**2))\n",
    "\n",
    "\n",
    "def get_rmse_on_batch(synthetic_data, test_dataset):\n",
    "    \"\"\"\n",
    "        Function that calculates the minimum RMS between \n",
    "        a batch of synthetic datapoints and a batch of test samples\n",
    "    \"\"\"\n",
    "    \n",
    "    rmse_all = []\n",
    "\n",
    "    for gen in synthetic_data:\n",
    "        rmse = np.inf\n",
    "        for test_datapoint in test_dataset:\n",
    "            current_rmse = get_rmse(gen, test_datapoint)\n",
    "            if current_rmse < rmse:\n",
    "                rmse = current_rmse\n",
    "        rmse_all.append(rmse)\n",
    "\n",
    "    return np.average(rmse_all) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent Root Mean Square Difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prmsd(synthetic_datapoint, original_datapoint):\n",
    "    \"\"\"\n",
    "        Function that calculates the percent root mean square \n",
    "        difference between two datapoints\n",
    "    \"\"\"\n",
    "    return np.sqrt(100 * (np.sum((synthetic_datapoint - original_datapoint)**2)) / (np.sum(synthetic_datapoint**2)))\n",
    "\n",
    "def get_prmsd_on_batch(synthetic_data, test_dataset):\n",
    "    \"\"\"\n",
    "        Function that calculates the minimum percent root mean square \n",
    "        difference between a batch of synthetic\n",
    "        datapoints and a batch of test samples\n",
    "    \"\"\"\n",
    "    \n",
    "    prmsd_all = []\n",
    "\n",
    "    for gen in synthetic_data:\n",
    "        prmsd = np.inf\n",
    "        for test_datapoint in test_dataset:\n",
    "            current_prmsd = get_prmsd(gen, test_datapoint)\n",
    "            if current_prmsd < prmsd:\n",
    "                prmsd = current_prmsd\n",
    "        prmsd_all.append(prmsd)\n",
    "\n",
    "    return np.average(prmsd_all) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(synthetic_datapoint, original_datapoint):\n",
    "    \"\"\"\n",
    "        Function that calculates the mean absolute\n",
    "        error between two datapoints\n",
    "    \"\"\"\n",
    "    return np.average(np.abs(synthetic_datapoint - original_datapoint))\n",
    "\n",
    "def get_mae_on_batch(synthetic_data, test_dataset):\n",
    "    \"\"\"\n",
    "        Function that calculates the minimum mean absolute\n",
    "        error between a batch of synthetic datapoints and a batch of test samples\n",
    "    \"\"\"\n",
    "    \n",
    "    mae_all = []\n",
    "\n",
    "    for gen in synthetic_data:\n",
    "        mae = np.inf\n",
    "        for test_datapoint in test_dataset:\n",
    "            current_mae = get_mae(gen, test_datapoint)\n",
    "            if current_mae < mae:\n",
    "                mae = current_mae\n",
    "        mae_all.append(mae)\n",
    "\n",
    "    return np.average(mae_all) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Performance Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = generate_data(generator, num_synthetic_to_gen= 10)\n",
    "rmse_ = get_rmse_on_batch(batch, test_data)\n",
    "prmsd_ = get_prmsd_on_batch(batch, test_data)\n",
    "mae_ = get_mae_on_batch(batch, test_data)\n",
    "print(\"RMSe at Testing Dataset: {}\".format(rmse_))\n",
    "print(\"PRMSD at Testing Dataset: {}\".format(prmsd_))\n",
    "print(\"MAE at Testing Dataset: {}\".format(mae_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Synthesized Data as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data, filepath):\n",
    "    \"\"\"\n",
    "        Function that takes in the data as numpy array,\n",
    "        converts to pandas dataframe and then saves the .csv file.\n",
    "    \"\"\"\n",
    "   # columns = [\"Column{}\".format(i) for i in range(data.shape[1])]\n",
    "    \n",
    "   \n",
    "    df = pd.DataFrame(data, columns= column_labels())\n",
    "    df.to_csv(filepath)\n",
    "\n",
    "def draw_generated_figures(data, folderpath):\n",
    "    \"\"\"\n",
    "        Function that takes in the generated batch of data\n",
    "        and saves the corresponding signal outputs as figures\n",
    "    \"\"\"\n",
    "    for i in range(data.shape[0]):\n",
    "        fig = plt.figure()\n",
    "        plt.plot(data[i].tolist(), 'r')\n",
    "        plt.savefig(folderpath + '/generated_sample_{:04d}.png'.format(i))\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating samples\n",
    "generated_batch = generate_data(generator, num_synthetic_to_gen=100)\n",
    "\n",
    "# Undoing any normalization that happened \n",
    "# generated_batch = ((generated_batch * std_dev) + means).astype(np.int32)     # Converting to ints\n",
    "save_data_to_csv(generated_batch, 'output/csv/samples100.csv')\n",
    "draw_generated_figures(generated_batch, 'output/generated_samples')\n",
    "print(\"Saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_generator = tf.keras.models.load_model('models/generator_model_A.ckpt')        # Load the model\n",
    "loaded_generator.compile()                                                          # Compile the model\n",
    "generated_data = generate_data(generator, num_synthetic_to_gen=1)\n",
    "print(generated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exporting the last 1000 checkpoints (in 2000 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the checkpoints as spectra\n",
    "for x in range(1000):\n",
    "    checkpoint.restore('./training_checkpoints/ckpt-' + str(x+1))\n",
    "    checkpoint.generator(noise)\n",
    "    generated_batch = generate_data(generator, num_synthetic_to_gen=1)\n",
    "    save_data_to_csv(generated_batch, 'output/cp/synth_o_raw_bz4_bn_GN00_model23_cp' + str(x+1) + '.csv')\n",
    "print(\"Saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd) # Sanity check: being in the correct directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the files\n",
    "\n",
    "os.chdir('./output/cp')\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
    "#export to csv\n",
    "\n",
    "combined_csv.to_csv('synth_o_raw_bz4_bn_GN00_model23_samples1000_cp.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(cwd) # return to the starting directory and..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "eded81c4a7c6917c9cbd3629f4297c0af6b02e3629b6d4cad1fc0bc42eaeccd9"
  },
  "kernelspec": {
   "display_name": "Python [conda env:gans]",
   "language": "python",
   "name": "conda-env-gans-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
